{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "CleanScanner - lightweight educational web vulnerability scanner\n",
        "Save as scanner.py and run: python scanner.py\n",
        "Default target: http://testphp.vulnweb.com/\n",
        "\"\"\"\n",
        "\n",
        "# Optional: in Colab or fresh env install:\n",
        "# !pip install requests beautifulsoup4\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin, urlparse, parse_qs\n",
        "from collections import deque\n",
        "import hashlib, ssl, socket, time\n",
        "from datetime import datetime\n",
        "from random import shuffle\n",
        "\n",
        "# colorama is optional; fallback to blank colors if missing\n",
        "try:\n",
        "    from colorama import Fore, init\n",
        "    init(autoreset=True)\n",
        "except Exception:\n",
        "    class Fore:\n",
        "        CYAN = \"\"\n",
        "        YELLOW = \"\"\n",
        "        RED = \"\"\n",
        "        GREEN = \"\"\n",
        "        BLUE = \"\"\n",
        "\n",
        "# ------------------------------\n",
        "# CONFIGURATION (tweak these)\n",
        "# ------------------------------\n",
        "START_URL = \"http://testphp.vulnweb.com/\"   # safe test target\n",
        "MAX_CRAWL_DEPTH = 1\n",
        "MAX_PARAM_URLS = 12\n",
        "REQUEST_TIMEOUT = 6\n",
        "CRAWL_USER_AGENT = \"CleanScanner/1.0 (edu)\"\n",
        "\n",
        "# payloads\n",
        "SQLI_PAYLOADS = [\"'\", \"' OR '1'='1\", \"' UNION SELECT NULL --\"]\n",
        "XSS_PAYLOADS = [\"<script>alert(1)</script>\", \"'><img src=x onerror=alert(1)>\", \"<svg/onload=alert(1)>\"]\n",
        "\n",
        "# OWASP mapping\n",
        "OWASP_MAPPING = {\n",
        "    \"BrokenAccess\": \"A01:2021 - Broken Access Control\",\n",
        "    \"SQLi\": \"A03:2021 - Injection\",\n",
        "    \"XSS\": \"A03:2021 - Injection (Cross-Site Scripting)\",\n",
        "    \"Headers\": \"A05:2021 - Security Misconfiguration\",\n",
        "    \"Crypto\": \"A02:2021 - Cryptographic Failures\",\n",
        "    \"InsecureDesign\": \"A04:2021 - Insecure Design\"\n",
        "}\n",
        "\n",
        "# findings container\n",
        "findings_list = []\n",
        "_findings_keys = set()\n",
        "\n",
        "# verbosity flag\n",
        "VERBOSE = False\n",
        "\n",
        "# A01 helpers/settings\n",
        "COMMON_ADMIN_PATHS = [\n",
        "    \"admin/\", \"administrator/\", \"admin/login.php\", \"admin.php\", \"login.php\",\n",
        "    \"user/login\", \"wp-admin/\", \"config.php\", \".git/\", \".env\", \"phpinfo.php\",\n",
        "    \"backup/\", \"server-status\", \"console/\", \"manage/\"\n",
        "]\n",
        "DIR_LISTING_MARKERS = [\"Index of /\", \"Parent Directory\", \"<title>Index of\", \"Directory listing for\"]\n",
        "DANGEROUS_HTTP_METHODS = {\"PUT\", \"DELETE\", \"TRACE\", \"TRACK\", \"PATCH\"}\n",
        "\n",
        "# IDOR offsets\n",
        "IDOR_PROBE_OFFSETS = (-2, -1, 1, 2)\n",
        "IDOR_MAX_EXAMPLES = 2\n",
        "\n",
        "# Role-based test: disabled by default (set True only for your lab)\n",
        "ENABLE_ROLE_TEST = False\n",
        "ROLE_TEST_CREDENTIALS = [(\"admin\", \"admin\"), (\"admin\", \"password\"), (\"admin\", \"123456\")]\n",
        "\n",
        "# ------------------------------\n",
        "# helpers\n",
        "# ------------------------------\n",
        "def logv(msg):\n",
        "    if VERBOSE:\n",
        "        print(Fore.BLUE + \"[DEBUG] \" + msg)\n",
        "\n",
        "def normalize_url_without_fragment(url):\n",
        "    p = urlparse(url)\n",
        "    return p._replace(fragment=\"\").geturl()\n",
        "\n",
        "def short_hash(text):\n",
        "    return hashlib.sha256(text.encode(\"utf-8\", errors=\"ignore\")).hexdigest()[:12]\n",
        "\n",
        "# ------------------------------\n",
        "# Crawler - BFS for parameterized URLs\n",
        "# ------------------------------\n",
        "def crawl_for_param_urls(start_url, max_depth=1, max_urls=12, same_domain_only=True):\n",
        "    parsed_start = urlparse(start_url)\n",
        "    base_domain = parsed_start.netloc\n",
        "    visited = set()\n",
        "    found_param_urls = []\n",
        "    queue = deque([(start_url, 0)])\n",
        "    headers = {\"User-Agent\": CRAWL_USER_AGENT}\n",
        "\n",
        "    while queue and len(found_param_urls) < max_urls:\n",
        "        current_url, depth = queue.popleft()\n",
        "        current_url = normalize_url_without_fragment(current_url)\n",
        "        if current_url in visited or depth > max_depth:\n",
        "            continue\n",
        "        visited.add(current_url)\n",
        "        logv(f\"Crawling: {current_url} (depth {depth})\")\n",
        "        try:\n",
        "            r = requests.get(current_url, timeout=REQUEST_TIMEOUT, headers=headers)\n",
        "            if r.status_code != 200:\n",
        "                logv(f\"Skipping non-200: {r.status_code} for {current_url}\")\n",
        "                continue\n",
        "            soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "            for a in soup.find_all(\"a\", href=True):\n",
        "                href = a[\"href\"].strip()\n",
        "                next_url = urljoin(current_url, href)\n",
        "                next_url = normalize_url_without_fragment(next_url)\n",
        "                parsed = urlparse(next_url)\n",
        "                if same_domain_only and parsed.netloc != base_domain:\n",
        "                    continue\n",
        "                if parsed.query:\n",
        "                    if next_url not in found_param_urls:\n",
        "                        found_param_urls.append(next_url)\n",
        "                        logv(f\"Found parameterized URL: {next_url}\")\n",
        "                        if len(found_param_urls) >= max_urls:\n",
        "                            break\n",
        "                if depth + 1 <= max_depth:\n",
        "                    queue.append((next_url, depth + 1))\n",
        "            time.sleep(0.05)\n",
        "        except Exception as e:\n",
        "            logv(f\"Crawl error for {current_url}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # dedupe preserving order\n",
        "    unique = []\n",
        "    for u in found_param_urls:\n",
        "        if u not in unique:\n",
        "            unique.append(u)\n",
        "    return unique\n",
        "\n",
        "# ------------------------------\n",
        "# Header check (A05)\n",
        "# ------------------------------\n",
        "def header_check(url):\n",
        "    try:\n",
        "        r = requests.get(url, timeout=REQUEST_TIMEOUT, headers={\"User-Agent\": CRAWL_USER_AGENT})\n",
        "        important = [\"Content-Security-Policy\", \"X-Frame-Options\", \"Strict-Transport-Security\", \"X-Content-Type-Options\"]\n",
        "        missing = [h for h in important if h not in r.headers]\n",
        "        if missing:\n",
        "            key = (\"Headers\", url, None)\n",
        "            if key not in _findings_keys:\n",
        "                _findings_keys.add(key)\n",
        "                findings_list.append({\n",
        "                    \"name\": \"Security Misconfiguration - Missing HTTP Headers\",\n",
        "                    \"type\": \"Headers\",\n",
        "                    \"url\": url,\n",
        "                    \"param\": None,\n",
        "                    \"payloads\": missing\n",
        "                })\n",
        "                logv(f\"Header issues for {url}: {missing}\")\n",
        "    except Exception as e:\n",
        "        logv(f\"Header check error for {url}: {e}\")\n",
        "\n",
        "# ------------------------------\n",
        "# SQLi check (A03)\n",
        "# ------------------------------\n",
        "def test_sqli(url):\n",
        "    params = list(parse_qs(urlparse(url).query).keys())\n",
        "    if not params:\n",
        "        return\n",
        "    base = url.split(\"?\")[0]\n",
        "    for p in params:\n",
        "        successful = []\n",
        "        for payload in SQLI_PAYLOADS:\n",
        "            try:\n",
        "                r = requests.get(base, params={p: payload}, timeout=REQUEST_TIMEOUT, headers={\"User-Agent\": CRAWL_USER_AGENT})\n",
        "                error_signs = [\"sql syntax\", \"mysql\", \"ORA-\", \"syntax error\", \"PDOException\"]\n",
        "                if any(e.lower() in r.text.lower() for e in error_signs):\n",
        "                    if payload not in successful:\n",
        "                        successful.append(payload)\n",
        "                    if len(successful) >= 2:\n",
        "                        break\n",
        "            except Exception as e:\n",
        "                logv(f\"SQLi request error: {e}\")\n",
        "        if successful:\n",
        "            key = (\"SQLi\", base, p)\n",
        "            if key not in _findings_keys:\n",
        "                _findings_keys.add(key)\n",
        "                findings_list.append({\n",
        "                    \"name\": \"SQL Injection\",\n",
        "                    \"type\": \"SQLi\",\n",
        "                    \"url\": base + \"?\" + p + \"=<value>\",\n",
        "                    \"param\": p,\n",
        "                    \"payloads\": successful\n",
        "                })\n",
        "                logv(f\"Saved SQLi finding: {base} param={p} payloads={successful}\")\n",
        "\n",
        "# ------------------------------\n",
        "# XSS check (A03)\n",
        "# ------------------------------\n",
        "def test_xss(url):\n",
        "    params = list(parse_qs(urlparse(url).query).keys())\n",
        "    if not params:\n",
        "        return\n",
        "    base = url.split(\"?\")[0]\n",
        "    for p in params:\n",
        "        successful = []\n",
        "        for payload in XSS_PAYLOADS:\n",
        "            try:\n",
        "                r = requests.get(base, params={p: payload}, timeout=REQUEST_TIMEOUT, headers={\"User-Agent\": CRAWL_USER_AGENT})\n",
        "                if payload in r.text or requests.utils.quote(payload) in r.text:\n",
        "                    if payload not in successful:\n",
        "                        successful.append(payload)\n",
        "                    if len(successful) >= 2:\n",
        "                        break\n",
        "            except Exception as e:\n",
        "                logv(f\"XSS request error: {e}\")\n",
        "        if successful:\n",
        "            key = (\"XSS\", base, p)\n",
        "            if key not in _findings_keys:\n",
        "                _findings_keys.add(key)\n",
        "                findings_list.append({\n",
        "                    \"name\": \"Cross-Site Scripting (Reflected)\",\n",
        "                    \"type\": \"XSS\",\n",
        "                    \"url\": base + \"?\" + p + \"=<value>\",\n",
        "                    \"param\": p,\n",
        "                    \"payloads\": successful\n",
        "                })\n",
        "                logv(f\"Saved XSS finding: {base} param={p} payloads={successful}\")\n",
        "\n",
        "# ------------------------------\n",
        "# IDOR detection (A01 basic)\n",
        "# ------------------------------\n",
        "def detect_idor(url, probe_offsets=IDOR_PROBE_OFFSETS, max_examples=IDOR_MAX_EXAMPLES):\n",
        "    parsed = urlparse(url)\n",
        "    qs = parse_qs(parsed.query)\n",
        "    if not qs:\n",
        "        return\n",
        "    base = parsed._replace(query=\"\").geturl()\n",
        "    for param, vals in qs.items():\n",
        "        val = vals[0]\n",
        "        try:\n",
        "            orig_int = int(val)\n",
        "        except Exception:\n",
        "            continue\n",
        "        try:\n",
        "            r0 = requests.get(base, params={param: orig_int}, timeout=REQUEST_TIMEOUT, headers={\"User-Agent\": CRAWL_USER_AGENT})\n",
        "            hash0 = short_hash(r0.text)\n",
        "        except Exception as e:\n",
        "            logv(f\"IDOR baseline error: {e}\")\n",
        "            continue\n",
        "        examples = []\n",
        "        for off in probe_offsets:\n",
        "            probe_val = orig_int + off\n",
        "            if probe_val < 0:\n",
        "                continue\n",
        "            try:\n",
        "                rp = requests.get(base, params={param: probe_val}, timeout=REQUEST_TIMEOUT, headers={\"User-Agent\": CRAWL_USER_AGENT})\n",
        "                if rp.status_code == 200:\n",
        "                    hashp = short_hash(rp.text)\n",
        "                    if hashp != hash0:\n",
        "                        examples.append((probe_val, rp.status_code))\n",
        "                        if len(examples) >= max_examples:\n",
        "                            break\n",
        "            except Exception as e:\n",
        "                logv(f\"IDOR probe error: {e}\")\n",
        "                continue\n",
        "        if examples:\n",
        "            key = (\"BrokenAccess\", base, param)\n",
        "            if key not in _findings_keys:\n",
        "                _findings_keys.add(key)\n",
        "                payload_examples = [str(e[0]) for e in examples]\n",
        "                findings_list.append({\n",
        "                    \"name\": \"Broken Access Control - Possible IDOR\",\n",
        "                    \"type\": \"BrokenAccess\",\n",
        "                    \"url\": base + \"?\" + param + \"=<id>\",\n",
        "                    \"param\": param,\n",
        "                    \"payloads\": payload_examples\n",
        "                })\n",
        "                logv(f\"Saved IDOR finding: {base} param={param} examples={payload_examples}\")\n",
        "\n",
        "# ------------------------------\n",
        "# A01 extended checks (forced browsing, methods, dir listing)\n",
        "# ------------------------------\n",
        "def forced_browsing_check(start_url, paths=COMMON_ADMIN_PATHS, max_checks=10):\n",
        "    parsed = urlparse(start_url)\n",
        "    base = f\"{parsed.scheme}://{parsed.netloc}/\"\n",
        "    checks = 0\n",
        "    headers = {\"User-Agent\": CRAWL_USER_AGENT}\n",
        "    for p in paths:\n",
        "        if checks >= max_checks:\n",
        "            break\n",
        "        target = urljoin(base, p)\n",
        "        try:\n",
        "            resp = requests.get(target, timeout=REQUEST_TIMEOUT, headers=headers, allow_redirects=True)\n",
        "            status = resp.status_code\n",
        "            body = resp.text or \"\"\n",
        "            if status == 200 and len(body) > 50:\n",
        "                key = (\"BrokenAccess\", target, None)\n",
        "                if key not in _findings_keys:\n",
        "                    _findings_keys.add(key)\n",
        "                    findings_list.append({\n",
        "                        \"name\": \"Broken Access Control - Forced Browsing (Exposed Page)\",\n",
        "                        \"type\": \"BrokenAccess\",\n",
        "                        \"url\": target,\n",
        "                        \"param\": None,\n",
        "                        \"payloads\": [f\"HTTP {status}\"]\n",
        "                    })\n",
        "            elif status in (401, 403):\n",
        "                lower = body.lower()\n",
        "                if \"login\" in lower or \"admin\" in lower or \"unauthorized\" in lower:\n",
        "                    key = (\"BrokenAccess\", target, None)\n",
        "                    if key not in _findings_keys:\n",
        "                        _findings_keys.add(key)\n",
        "                        findings_list.append({\n",
        "                            \"name\": \"Broken Access Control - Forced Browsing (Auth Gate)\",\n",
        "                            \"type\": \"BrokenAccess\",\n",
        "                            \"url\": target,\n",
        "                            \"param\": None,\n",
        "                            \"payloads\": [f\"HTTP {status} - login/admin markers\"]\n",
        "                        })\n",
        "            checks += 1\n",
        "        except Exception as e:\n",
        "            logv(f\"forced_browsing_check error for {target}: {e}\")\n",
        "            checks += 1\n",
        "            continue\n",
        "\n",
        "def check_insecure_http_methods(start_url, paths_to_check=None, max_checks=6):\n",
        "    headers = {\"User-Agent\": CRAWL_USER_AGENT}\n",
        "    parsed = urlparse(start_url)\n",
        "    base = f\"{parsed.scheme}://{parsed.netloc}\"\n",
        "    targets = []\n",
        "    if paths_to_check:\n",
        "        for p in paths_to_check[:max_checks]:\n",
        "            targets.append(urljoin(base + \"/\", p))\n",
        "    else:\n",
        "        targets = [base + \"/\", base + \"/api/\", base + \"/admin/\"]\n",
        "    for t in targets:\n",
        "        try:\n",
        "            r = requests.options(t, timeout=REQUEST_TIMEOUT, headers=headers)\n",
        "            allow = r.headers.get(\"Allow\", \"\") or r.headers.get(\"allow\", \"\")\n",
        "            if allow:\n",
        "                allowed = {m.strip().upper() for m in allow.split(\",\") if m.strip()}\n",
        "                dangerous = allowed.intersection(DANGEROUS_HTTP_METHODS)\n",
        "                if dangerous:\n",
        "                    key = (\"BrokenAccess\", t, None)\n",
        "                    if key not in _findings_keys:\n",
        "                        _findings_keys.add(key)\n",
        "                        findings_list.append({\n",
        "                            \"name\": \"Broken Access Control - Insecure HTTP Methods Allowed\",\n",
        "                            \"type\": \"BrokenAccess\",\n",
        "                            \"url\": t,\n",
        "                            \"param\": None,\n",
        "                            \"payloads\": [\", \".join(sorted(dangerous))]\n",
        "                        })\n",
        "        except Exception as e:\n",
        "            logv(f\"check_insecure_http_methods error for {t}: {e}\")\n",
        "            continue\n",
        "\n",
        "def check_directory_listing(start_url, sample_paths=None, max_checks=8):\n",
        "    headers = {\"User-Agent\": CRAWL_USER_AGENT}\n",
        "    parsed = urlparse(start_url)\n",
        "    base = f\"{parsed.scheme}://{parsed.netloc}/\"\n",
        "    candidates = []\n",
        "    if sample_paths:\n",
        "        for u in sample_paths:\n",
        "            p = urlparse(u).path\n",
        "            if p.endswith(\"/\"):\n",
        "                dirurl = urljoin(base, p)\n",
        "            else:\n",
        "                dirurl = urljoin(base, \"/\".join(p.split(\"/\")[:-1]) + \"/\")\n",
        "            if dirurl not in candidates:\n",
        "                candidates.append(dirurl)\n",
        "    else:\n",
        "        candidates = [base]\n",
        "    checks = 0\n",
        "    for d in candidates:\n",
        "        if checks >= max_checks:\n",
        "            break\n",
        "        try:\n",
        "            r = requests.get(d, timeout=REQUEST_TIMEOUT, headers=headers)\n",
        "            body = (r.text or \"\").lower()\n",
        "            if any(marker.lower() in body for marker in DIR_LISTING_MARKERS):\n",
        "                key = (\"BrokenAccess\", d, None)\n",
        "                if key not in _findings_keys:\n",
        "                    _findings_keys.add(key)\n",
        "                    findings_list.append({\n",
        "                        \"name\": \"Broken Access Control - Directory Listing Enabled\",\n",
        "                        \"type\": \"BrokenAccess\",\n",
        "                        \"url\": d,\n",
        "                        \"param\": None,\n",
        "                        \"payloads\": [\"Directory listing detected\"]\n",
        "                    })\n",
        "            checks += 1\n",
        "        except Exception as e:\n",
        "            logv(f\"check_directory_listing error for {d}: {e}\")\n",
        "            checks += 1\n",
        "            continue\n",
        "\n",
        "# ------------------------------\n",
        "# Optional: role-based access test (disabled by default)\n",
        "# ------------------------------\n",
        "def role_based_access_test(start_url, paths=None, credentials=None, max_checks=6):\n",
        "    if not ENABLE_ROLE_TEST:\n",
        "        logv(\"Role-based tests disabled (ENABLE_ROLE_TEST=False)\")\n",
        "        return\n",
        "    if not credentials:\n",
        "        credentials = ROLE_TEST_CREDENTIALS\n",
        "    headers = {\"User-Agent\": CRAWL_USER_AGENT}\n",
        "    parsed = urlparse(start_url)\n",
        "    base = f\"{parsed.scheme}://{parsed.netloc}\"\n",
        "    targets = [base + \"/admin/\", base + \"/login.php\", base + \"/dashboard/\"]\n",
        "    if paths:\n",
        "        targets = (targets + paths)[:max_checks]\n",
        "    for t in targets:\n",
        "        try:\n",
        "            r = requests.get(t, timeout=REQUEST_TIMEOUT, headers=headers, allow_redirects=True)\n",
        "            public_status = r.status_code\n",
        "            public_hash = short_hash(r.text)\n",
        "        except Exception:\n",
        "            continue\n",
        "        for (u, p) in credentials:\n",
        "            try:\n",
        "                ra = requests.get(t, auth=(u, p), timeout=REQUEST_TIMEOUT, headers=headers, allow_redirects=True)\n",
        "                if ra.status_code == 200:\n",
        "                    auth_hash = short_hash(ra.text)\n",
        "                    if public_status in (401,403) or auth_hash != public_hash:\n",
        "                        key = (\"BrokenAccess\", t, None)\n",
        "                        if key not in _findings_keys:\n",
        "                            _findings_keys.add(key)\n",
        "                            findings_list.append({\n",
        "                                \"name\": \"Broken Access Control - Role-based/Auth test succeeded\",\n",
        "                                \"type\": \"BrokenAccess\",\n",
        "                                \"url\": t,\n",
        "                                \"param\": None,\n",
        "                                \"payloads\": [f\"Creds: {u}/{p} -> HTTP {ra.status_code}\"]\n",
        "                            })\n",
        "                time.sleep(0.2)\n",
        "            except Exception as e:\n",
        "                logv(f\"role_based_access_test error for {t} with {u}: {e}\")\n",
        "                continue\n",
        "\n",
        "# ------------------------------\n",
        "# Crypto checks (A02)\n",
        "# ------------------------------\n",
        "def check_crypto_tls(start_url):\n",
        "    parsed = urlparse(start_url)\n",
        "    host = parsed.hostname\n",
        "    port = 443\n",
        "    cert_issues = []\n",
        "    uses_https = False\n",
        "    try:\n",
        "        resp = requests.get(start_url, timeout=REQUEST_TIMEOUT, allow_redirects=True, headers={\"User-Agent\": CRAWL_USER_AGENT})\n",
        "        final_url = resp.url\n",
        "        uses_https = final_url.startswith(\"https://\")\n",
        "    except Exception as e:\n",
        "        logv(f\"HTTPS availability check error: {e}\")\n",
        "    if uses_https:\n",
        "        try:\n",
        "            ctx = ssl.create_default_context()\n",
        "            with socket.create_connection((host, port), timeout=REQUEST_TIMEOUT) as sock:\n",
        "                with ctx.wrap_socket(sock, server_hostname=host) as ssock:\n",
        "                    cert = ssock.getpeercert()\n",
        "                    notAfter = cert.get('notAfter')\n",
        "                    if notAfter:\n",
        "                        try:\n",
        "                            exp = datetime.strptime(notAfter, \"%b %d %H:%M:%S %Y %Z\")\n",
        "                            days_left = (exp - datetime.utcnow()).days\n",
        "                            if days_left < 0:\n",
        "                                cert_issues.append(f\"Certificate expired {abs(days_left)} days ago\")\n",
        "                            elif days_left < 90:\n",
        "                                cert_issues.append(f\"Certificate expires in {days_left} days (consider renewal)\")\n",
        "                        except Exception:\n",
        "                            logv(f\"Could not parse cert expiry: {notAfter}\")\n",
        "                    issuer = cert.get('issuer')\n",
        "                    subject = cert.get('subject')\n",
        "                    if issuer == subject:\n",
        "                        cert_issues.append(\"Certificate appears self-signed (issuer == subject)\")\n",
        "        except Exception as e:\n",
        "            cert_issues.append(f\"TLS handshake failed or certificate not retrievable: {e}\")\n",
        "            logv(f\"TLS handshake error: {e}\")\n",
        "    else:\n",
        "        cert_issues.append(\"Site does not redirect to HTTPS or HTTPS not available\")\n",
        "    try:\n",
        "        r = requests.get(start_url, timeout=REQUEST_TIMEOUT, headers={\"User-Agent\": CRAWL_USER_AGENT})\n",
        "        if 'Strict-Transport-Security' not in r.headers:\n",
        "            cert_issues.append(\"Missing Strict-Transport-Security header (HSTS)\")\n",
        "    except Exception:\n",
        "        pass\n",
        "    if cert_issues:\n",
        "        key = (\"Crypto\", start_url, None)\n",
        "        if key not in _findings_keys:\n",
        "            _findings_keys.add(key)\n",
        "            findings_list.append({\n",
        "                \"name\": \"Cryptographic Failures / TLS Issues\",\n",
        "                \"type\": \"Crypto\",\n",
        "                \"url\": start_url,\n",
        "                \"param\": None,\n",
        "                \"payloads\": cert_issues\n",
        "            })\n",
        "\n",
        "# ------------------------------\n",
        "# Insecure design heuristics (A04)\n",
        "# ------------------------------\n",
        "def check_insecure_design(start_url, max_pages=3):\n",
        "    sensitive_terms = [\"password\", \"secret\", \"api_key\", \"apikey\", \"token\", \"access_token\", \"private_key\", \"aws_secret\"]\n",
        "    csrf_like_names = [\"csrf\", \"token\", \"_csrf\", \"authenticity_token\", \"csrf_token\"]\n",
        "    headers_issues = []\n",
        "    page_issues = []\n",
        "    try:\n",
        "        r = requests.get(start_url, timeout=REQUEST_TIMEOUT, headers={\"User-Agent\": CRAWL_USER_AGENT})\n",
        "        for h in [\"Server\", \"X-Powered-By\"]:\n",
        "            if h in r.headers:\n",
        "                headers_issues.append(f\"Header leaks info: {h}: {r.headers.get(h)}\")\n",
        "    except Exception:\n",
        "        pass\n",
        "    to_visit = [start_url]\n",
        "    visited = set()\n",
        "    count = 0\n",
        "    while to_visit and count < max_pages:\n",
        "        url = to_visit.pop(0)\n",
        "        if url in visited:\n",
        "            continue\n",
        "        visited.add(url)\n",
        "        try:\n",
        "            r = requests.get(url, timeout=REQUEST_TIMEOUT, headers={\"User-Agent\": CRAWL_USER_AGENT})\n",
        "            if r.status_code != 200:\n",
        "                continue\n",
        "            soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "            text = r.text.lower()\n",
        "            found_terms = [t for t in sensitive_terms if t in text]\n",
        "            if found_terms:\n",
        "                page_issues.append(f\"Sensitive terms present on {url}: {', '.join(found_terms)}\")\n",
        "            for form in soup.find_all(\"form\"):\n",
        "                method = (form.get(\"method\") or \"\").lower()\n",
        "                hidden_names = [inp.get(\"name\",\"\").lower() for inp in form.find_all(\"input\", type=\"hidden\")]\n",
        "                has_csrf = any(any(cs in hn for hn in hidden_names) for cs in csrf_like_names)\n",
        "                if method == \"post\" and not has_csrf:\n",
        "                    page_issues.append(f\"POST form without apparent CSRF token on {url}\")\n",
        "                for pwd in form.find_all(\"input\", {\"type\":\"password\"}):\n",
        "                    ac = pwd.get(\"autocomplete\")\n",
        "                    if ac is None or ac.lower() != \"off\":\n",
        "                        page_issues.append(f\"Password input on {url} without autocomplete='off'\")\n",
        "            for a in soup.find_all(\"a\", href=True):\n",
        "                next_url = urljoin(url, a[\"href\"].strip())\n",
        "                if urlparse(next_url).netloc == urlparse(start_url).netloc and next_url not in visited:\n",
        "                    to_visit.append(next_url)\n",
        "            count += 1\n",
        "            time.sleep(0.05)\n",
        "        except Exception as e:\n",
        "            logv(f\"Insecure design page fetch error: {e}\")\n",
        "            continue\n",
        "    all_issues = headers_issues + page_issues\n",
        "    if all_issues:\n",
        "        key = (\"InsecureDesign\", start_url, None)\n",
        "        if key not in _findings_keys:\n",
        "            _findings_keys.add(key)\n",
        "            findings_list.append({\n",
        "                \"name\": \"Insecure Design - Heuristic Findings\",\n",
        "                \"type\": \"InsecureDesign\",\n",
        "                \"url\": start_url,\n",
        "                \"param\": None,\n",
        "                \"payloads\": all_issues\n",
        "            })\n",
        "\n",
        "# ------------------------------\n",
        "# Report printer (grouped)\n",
        "# ------------------------------\n",
        "def print_grouped_report(start_url):\n",
        "    print(\"\\n\" + \"-\"*60)\n",
        "    print(f\"Vulnerability Report for: {start_url}\")\n",
        "    print(\"-\"*60 + \"\\n\")\n",
        "\n",
        "    broken = [f for f in findings_list if f[\"type\"] == \"BrokenAccess\"]\n",
        "    sqli = [f for f in findings_list if f[\"type\"] == \"SQLi\"]\n",
        "    xss = [f for f in findings_list if f[\"type\"] == \"XSS\"]\n",
        "    crypto = [f for f in findings_list if f[\"type\"] == \"Crypto\"]\n",
        "    insecure = [f for f in findings_list if f[\"type\"] == \"InsecureDesign\"]\n",
        "    headers = [f for f in findings_list if f[\"type\"] == \"Headers\"]\n",
        "\n",
        "    if broken:\n",
        "        print(\"========== CATEGORY: Broken Access Control (A01) ==========\\n\")\n",
        "        for f in broken:\n",
        "            print(f\"NAME OF VULN: {f['name']}\")\n",
        "            print(f\"TYPE OF VULN BASED ON WHICH VULN OF OWASP TOP 10: {OWASP_MAPPING.get(f['type'])}\")\n",
        "            print(f\"AFFECTED URL: {f['url']}\")\n",
        "            if f['param']:\n",
        "                print(f\"AFFECTED PARAMETER: {f['param']}\")\n",
        "            print(\"PAYLOADS USED: \" + \", \".join(f['payloads']))\n",
        "            print(\"\\n\" + \"-\"*40 + \"\\n\")\n",
        "\n",
        "    if sqli or xss:\n",
        "        print(\"========== CATEGORY: Injection (A03) ==========\\n\")\n",
        "        if sqli:\n",
        "            print(\"---- Subgroup: SQL Injection ----\\n\")\n",
        "            for f in sqli:\n",
        "                print(f\"NAME OF VULN: {f['name']}\")\n",
        "                print(f\"TYPE OF VULN BASED ON WHICH VULN OF OWASP TOP 10: {OWASP_MAPPING.get(f['type'])}\")\n",
        "                print(f\"AFFECTED URL: {f['url']}\")\n",
        "                print(f\"AFFECTED PARAMETER: {f['param']}\")\n",
        "                print(\"PAYLOADS USED: \" + \", \".join(f['payloads']))\n",
        "                print(\"\\n\" + \"-\"*40 + \"\\n\")\n",
        "        if xss:\n",
        "            print(\"---- Subgroup: Cross-Site Scripting (XSS) ----\\n\")\n",
        "            for f in xss:\n",
        "                print(f\"NAME OF VULN: {f['name']}\")\n",
        "                print(f\"TYPE OF VULN BASED ON WHICH VULN OF OWASP TOP 10: {OWASP_MAPPING.get(f['type'])}\")\n",
        "                print(f\"AFFECTED URL: {f['url']}\")\n",
        "                print(f\"AFFECTED PARAMETER: {f['param']}\")\n",
        "                print(\"PAYLOADS USED: \" + \", \".join(f['payloads']))\n",
        "                print(\"\\n\" + \"-\"*40 + \"\\n\")\n",
        "\n",
        "    if crypto:\n",
        "        print(\"========== CATEGORY: Cryptographic Failures (A02) ==========\\n\")\n",
        "        for f in crypto:\n",
        "            print(f\"NAME OF VULN: {f['name']}\")\n",
        "            print(f\"TYPE OF VULN BASED ON WHICH VULN OF OWASP TOP 10: {OWASP_MAPPING.get(f['type'])}\")\n",
        "            print(f\"AFFECTED URL: {f['url']}\")\n",
        "            print(\"DETAILS: \" + \"; \".join(f['payloads']))\n",
        "            print(\"\\n\" + \"-\"*40 + \"\\n\")\n",
        "\n",
        "    if insecure:\n",
        "        print(\"========== CATEGORY: Insecure Design (A04) ==========\\n\")\n",
        "        for f in insecure:\n",
        "            print(f\"NAME OF VULN: {f['name']}\")\n",
        "            print(f\"TYPE OF VULN BASED ON WHICH VULN OF OWASP TOP 10: {OWASP_MAPPING.get(f['type'])}\")\n",
        "            print(f\"AFFECTED URL: {f['url']}\")\n",
        "            print(\"DETAILS:\")\n",
        "            for d in f['payloads']:\n",
        "                print(\"  - \" + d)\n",
        "            print(\"\\n\" + \"-\"*40 + \"\\n\")\n",
        "\n",
        "    if headers:\n",
        "        print(\"========== CATEGORY: Security Misconfiguration (A05) ==========\\n\")\n",
        "        for f in headers:\n",
        "            print(f\"NAME OF VULN: {f['name']}\")\n",
        "            print(f\"TYPE OF VULN BASED ON WHICH VULN OF OWASP TOP 10: {OWASP_MAPPING.get(f['type'])}\")\n",
        "            print(f\"AFFECTED URL: {f['url']}\")\n",
        "            print(\"MISSING HEADERS: \" + \", \".join(f['payloads']))\n",
        "            print(\"\\n\" + \"-\"*40 + \"\\n\")\n",
        "\n",
        "    print(\"Summary:\")\n",
        "    print(f\"  Broken Access findings : {len(broken)}\")\n",
        "    print(f\"  SQLi findings          : {len(sqli)}\")\n",
        "    print(f\"  XSS findings           : {len(xss)}\")\n",
        "    print(f\"  Crypto issues          : {len(crypto)}\")\n",
        "    print(f\"  Insecure design issues : {len(insecure)}\")\n",
        "    print(f\"  Header issues          : {len(headers)}\")\n",
        "    print(\"-\"*60 + \"\\n\")\n",
        "\n",
        "# ------------------------------\n",
        "# Orchestration\n",
        "# ------------------------------\n",
        "def run_full_scanner(start_url=START_URL, max_depth=MAX_CRAWL_DEPTH, max_param_urls=MAX_PARAM_URLS, verbose=False):\n",
        "    global VERBOSE\n",
        "    VERBOSE = verbose\n",
        "    findings_list.clear()\n",
        "    _findings_keys.clear()\n",
        "\n",
        "    print(Fore.CYAN + f\"[*] Scanning target: {start_url}\\n\")\n",
        "\n",
        "    # header & crypto & insecure-design quick checks\n",
        "    header_check(start_url)\n",
        "    check_crypto_tls(start_url)\n",
        "    check_insecure_design(start_url, max_pages=3)\n",
        "\n",
        "    # crawl for parameterized URLs\n",
        "    param_urls = crawl_for_param_urls(start_url, max_depth=max_depth, max_urls=max_param_urls)\n",
        "    if not param_urls:\n",
        "        print(Fore.YELLOW + \"[!] No parameterized URLs found by crawler.\")\n",
        "        print_grouped_report(start_url)\n",
        "        return\n",
        "\n",
        "    # per-URL tests\n",
        "    for u in param_urls:\n",
        "        test_sqli(u)\n",
        "        test_xss(u)\n",
        "        detect_idor(u)\n",
        "\n",
        "    # extended A01 checks (forced browsing, methods, dir listing)\n",
        "    # small wrapper to call three functions\n",
        "    def _run_a01_extended(s, p):\n",
        "        forced_browsing_check(s, max_checks=8)\n",
        "        check_insecure_http_methods(s, paths_to_check=[\"/\",\"/admin/\",\"/api/\"], max_checks=5)\n",
        "        check_directory_listing(s, sample_paths=p, max_checks=6)\n",
        "    _run_a01_extended(start_url, param_urls)\n",
        "\n",
        "    # optional role-based tests (disabled by default)\n",
        "    role_based_access_test(start_url, paths=param_urls[:6], credentials=ROLE_TEST_CREDENTIALS, max_checks=6)\n",
        "\n",
        "    print_grouped_report(start_url)\n",
        "\n",
        "# ------------------------------\n",
        "# Run when executed directly\n",
        "# ------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # Set verbose=True for debug prints\n",
        "    run_full_scanner(start_url=START_URL, max_depth=MAX_CRAWL_DEPTH, max_param_urls=MAX_PARAM_URLS, verbose=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbdPH5vSOmPU",
        "outputId": "9f29eb68-07d6-4db8-a5c9-c5cfb77d7a67"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Scanning target: http://testphp.vulnweb.com/\n",
            "\n",
            "\n",
            "------------------------------------------------------------\n",
            "Vulnerability Report for: http://testphp.vulnweb.com/\n",
            "------------------------------------------------------------\n",
            "\n",
            "========== CATEGORY: Broken Access Control (A01) ==========\n",
            "\n",
            "NAME OF VULN: Broken Access Control - Possible IDOR\n",
            "TYPE OF VULN BASED ON WHICH VULN OF OWASP TOP 10: A01:2021 - Broken Access Control\n",
            "AFFECTED URL: http://testphp.vulnweb.com/listproducts.php?cat=<id>\n",
            "AFFECTED PARAMETER: cat\n",
            "PAYLOADS USED: 0, 2\n",
            "\n",
            "----------------------------------------\n",
            "\n",
            "NAME OF VULN: Broken Access Control - Possible IDOR\n",
            "TYPE OF VULN BASED ON WHICH VULN OF OWASP TOP 10: A01:2021 - Broken Access Control\n",
            "AFFECTED URL: http://testphp.vulnweb.com/artists.php?artist=<id>\n",
            "AFFECTED PARAMETER: artist\n",
            "PAYLOADS USED: 0, 2\n",
            "\n",
            "----------------------------------------\n",
            "\n",
            "NAME OF VULN: Broken Access Control - Possible IDOR\n",
            "TYPE OF VULN BASED ON WHICH VULN OF OWASP TOP 10: A01:2021 - Broken Access Control\n",
            "AFFECTED URL: http://testphp.vulnweb.com/hpp/?pp=<id>\n",
            "AFFECTED PARAMETER: pp\n",
            "PAYLOADS USED: 10, 11\n",
            "\n",
            "----------------------------------------\n",
            "\n",
            "NAME OF VULN: Broken Access Control - Forced Browsing (Exposed Page)\n",
            "TYPE OF VULN BASED ON WHICH VULN OF OWASP TOP 10: A01:2021 - Broken Access Control\n",
            "AFFECTED URL: http://testphp.vulnweb.com/admin/\n",
            "PAYLOADS USED: HTTP 200\n",
            "\n",
            "----------------------------------------\n",
            "\n",
            "NAME OF VULN: Broken Access Control - Forced Browsing (Exposed Page)\n",
            "TYPE OF VULN BASED ON WHICH VULN OF OWASP TOP 10: A01:2021 - Broken Access Control\n",
            "AFFECTED URL: http://testphp.vulnweb.com/login.php\n",
            "PAYLOADS USED: HTTP 200\n",
            "\n",
            "----------------------------------------\n",
            "\n",
            "========== CATEGORY: Injection (A03) ==========\n",
            "\n",
            "---- Subgroup: SQL Injection ----\n",
            "\n",
            "NAME OF VULN: SQL Injection\n",
            "TYPE OF VULN BASED ON WHICH VULN OF OWASP TOP 10: A03:2021 - Injection\n",
            "AFFECTED URL: http://testphp.vulnweb.com/listproducts.php?cat=<value>\n",
            "AFFECTED PARAMETER: cat\n",
            "PAYLOADS USED: ', ' OR '1'='1\n",
            "\n",
            "----------------------------------------\n",
            "\n",
            "NAME OF VULN: SQL Injection\n",
            "TYPE OF VULN BASED ON WHICH VULN OF OWASP TOP 10: A03:2021 - Injection\n",
            "AFFECTED URL: http://testphp.vulnweb.com/artists.php?artist=<value>\n",
            "AFFECTED PARAMETER: artist\n",
            "PAYLOADS USED: ', ' OR '1'='1\n",
            "\n",
            "----------------------------------------\n",
            "\n",
            "---- Subgroup: Cross-Site Scripting (XSS) ----\n",
            "\n",
            "NAME OF VULN: Cross-Site Scripting (Reflected)\n",
            "TYPE OF VULN BASED ON WHICH VULN OF OWASP TOP 10: A03:2021 - Injection (Cross-Site Scripting)\n",
            "AFFECTED URL: http://testphp.vulnweb.com/listproducts.php?cat=<value>\n",
            "AFFECTED PARAMETER: cat\n",
            "PAYLOADS USED: <script>alert(1)</script>, '><img src=x onerror=alert(1)>\n",
            "\n",
            "----------------------------------------\n",
            "\n",
            "NAME OF VULN: Cross-Site Scripting (Reflected)\n",
            "TYPE OF VULN BASED ON WHICH VULN OF OWASP TOP 10: A03:2021 - Injection (Cross-Site Scripting)\n",
            "AFFECTED URL: http://testphp.vulnweb.com/artists.php?artist=<value>\n",
            "AFFECTED PARAMETER: artist\n",
            "PAYLOADS USED: <script>alert(1)</script>, '><img src=x onerror=alert(1)>\n",
            "\n",
            "----------------------------------------\n",
            "\n",
            "NAME OF VULN: Cross-Site Scripting (Reflected)\n",
            "TYPE OF VULN BASED ON WHICH VULN OF OWASP TOP 10: A03:2021 - Injection (Cross-Site Scripting)\n",
            "AFFECTED URL: http://testphp.vulnweb.com/hpp/?pp=<value>\n",
            "AFFECTED PARAMETER: pp\n",
            "PAYLOADS USED: <script>alert(1)</script>, '><img src=x onerror=alert(1)>\n",
            "\n",
            "----------------------------------------\n",
            "\n",
            "========== CATEGORY: Cryptographic Failures (A02) ==========\n",
            "\n",
            "NAME OF VULN: Cryptographic Failures / TLS Issues\n",
            "TYPE OF VULN BASED ON WHICH VULN OF OWASP TOP 10: A02:2021 - Cryptographic Failures\n",
            "AFFECTED URL: http://testphp.vulnweb.com/\n",
            "DETAILS: Site does not redirect to HTTPS or HTTPS not available; Missing Strict-Transport-Security header (HSTS)\n",
            "\n",
            "----------------------------------------\n",
            "\n",
            "========== CATEGORY: Insecure Design (A04) ==========\n",
            "\n",
            "NAME OF VULN: Insecure Design - Heuristic Findings\n",
            "TYPE OF VULN BASED ON WHICH VULN OF OWASP TOP 10: A04:2021 - Insecure Design\n",
            "AFFECTED URL: http://testphp.vulnweb.com/\n",
            "DETAILS:\n",
            "  - Header leaks info: Server: nginx/1.19.0\n",
            "  - Header leaks info: X-Powered-By: PHP/5.6.40-38+ubuntu20.04.1+deb.sury.org+1\n",
            "  - POST form without apparent CSRF token on http://testphp.vulnweb.com/\n",
            "  - POST form without apparent CSRF token on http://testphp.vulnweb.com/index.php\n",
            "  - POST form without apparent CSRF token on http://testphp.vulnweb.com/categories.php\n",
            "\n",
            "----------------------------------------\n",
            "\n",
            "========== CATEGORY: Security Misconfiguration (A05) ==========\n",
            "\n",
            "NAME OF VULN: Security Misconfiguration - Missing HTTP Headers\n",
            "TYPE OF VULN BASED ON WHICH VULN OF OWASP TOP 10: A05:2021 - Security Misconfiguration\n",
            "AFFECTED URL: http://testphp.vulnweb.com/\n",
            "MISSING HEADERS: Content-Security-Policy, X-Frame-Options, Strict-Transport-Security, X-Content-Type-Options\n",
            "\n",
            "----------------------------------------\n",
            "\n",
            "Summary:\n",
            "  Broken Access findings : 5\n",
            "  SQLi findings          : 2\n",
            "  XSS findings           : 3\n",
            "  Crypto issues          : 1\n",
            "  Insecure design issues : 1\n",
            "  Header issues          : 1\n",
            "------------------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    }
  ]
}